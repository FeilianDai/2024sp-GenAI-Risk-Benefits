---
layout: post
title: Introduction
lecture: S0-Intro
lectureVersion: current
extraContent: 
notes: team-1
video:  team-2
tags:
- 1Basic
---


### Basics of LLM 

#### What is NLP?
- Typical NLP tasks / Challenges / Pipeline
- f() on natural language
  + Before Deep NLP (Pre 2012) • (BOW / LSI / Topic Modeling LDA )
  + Word2Vec (2013-2016) • (GloVe/ FastText)
  + Recurrent NN (2014-2016) • LSTM
  + Seq2Seq
  + Attention 
  + Self-Attention (2016 – now )
  + Transformer (attention only Seq2Seq)
  + BERT / RoBERTa/ XLNet/ GPT / ...

### Basics of recent LLM 
  + Emergent Abilities of Large Language Models | [ URL](https://arxiv.org/abs/2206.07682) | "an ability to be emergent if it is not present in smaller models but is present in larger models. Thus, emergent abilities cannot be predicted simply by extrapolating the performance of smaller models."|
  + Language Models are Few-Shot Learners | [ URL](https://arxiv.org/abs/2005.14165) | "GPT-3, 175B autoregerssive LLM;  show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches."|

