---
layout: post
title: Survey LLMs and Multimodal FMs
lecture: S0-Intro
lectureVersion: next
extraContent: 
notes: instructor
video: on llm basics
tags:
- 1Basic
---

In this session, our readings cover: 




## Readings: 

### Basics of LLM 

#### What is NLP?
- Typical NLP tasks / Challenges / Pipeline
- f() on natural language
  + Before Deep NLP (Pre 2012) • (BOW / LSI / Topic Modeling LDA )
  + Word2Vec (2013-2016) • (GloVe/ FastText)
  + Recurrent NN (2014-2016) • LSTM
  + Seq2Seq
  + Attention 
  + Self-Attention (2016 – now )
  + Transformer (attention only Seq2Seq)
  + BERT / RoBERTa/ XLNet/ GPT / ...

#### Emergent Abilities of Large Language Models 
  + [ URL](https://arxiv.org/abs/2206.07682) 
  + "an ability to be emergent if it is not present in smaller models but is present in larger models. Thus, emergent abilities cannot be predicted simply by extrapolating the performance of smaller models."|

#### Language Models are Few-Shot Learners 
  + [ URL](https://arxiv.org/abs/2005.14165) 
  + "GPT-3, 175B autoregerssive LLM;  show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches."|



### A survey of Generative AI Applications
+ https://arxiv.org/abs/2306.02781
+ Roberto Gozalo-Brizuela, Eduardo C. Garrido-Merchán
+ Generative AI has experienced remarkable growth in recent years, leading to a wide array of applications across diverse domains. In this paper, we present a comprehensive survey of more than 350 generative AI applications, providing a structured taxonomy and concise descriptions of various unimodal and even multimodal generative AIs. The survey is organized into sections, covering a wide range of unimodal generative AI applications such as text, images, video, gaming and brain information. Our survey aims to serve as a valuable resource for researchers and practitioners to navigate the rapidly expanding landscape of generative AI, facilitating a better understanding of the current state-of-the-art and fostering further innovation in the field.

### Generative AI: Perspectives from Stanford HAI
 + https://hai.stanford.edu/generative-ai-perspectives-stanford-hai 

### A Survey of Large Language Models
 + https://arxiv.org/abs/2303.18223 
 + Language is essentially a complex, intricate system of human expressions governed by grammatical rules. It poses a significant challenge to develop capable AI algorithms for comprehending and grasping a language. As a major approach, language modeling has been widely studied for language understanding and generation in the past two decades, evolving from statistical language models to neural language models. Recently, pre-trained language models (PLMs) have been proposed by pre-training Transformer models over large-scale corpora, showing strong capabilities in solving various NLP tasks. Since researchers have found that model scaling can lead to performance improvement, they further study the scaling effect by increasing the model size to an even larger size. Interestingly, when the parameter scale exceeds a certain level, these enlarged language models not only achieve a significant performance improvement but also show some special abilities that are not present in small-scale language models. To discriminate the difference in parameter scale, the research community has coined the term large language models (LLM) for the PLMs of significant size. Recently, the research on LLMs has been largely advanced by both academia and industry, and a remarkable progress is the launch of ChatGPT, which has attracted widespread attention from society. The technical evolution of LLMs has been making an important impact on the entire AI community, which would revolutionize the way how we develop and use AI algorithms. In this survey, we review the recent advances of LLMs by introducing the background, key findings, and mainstream techniques. In particular, we focus on four major aspects of LLMs, namely pre-training, adaptation tuning, utilization, and capacity evaluation. Besides, we also summarize the available resources for developing LLMs and discuss the remaining issues for future directions.



