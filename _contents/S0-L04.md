---
layout: post
title: GenAI Guardrails 
lecture: 
lectureVersion: next
extraContent: 
notes: team-2
video: team-3
tags:
- Protect
---

In this session, our readings cover: 

## Required Readings: 

### Llama Guard: LLM-based Input-Output Safeguard for Human-AI Conversations
  + https://arxiv.org/abs/2312.06674
  + We introduce Llama Guard, an LLM-based input-output safeguard model geared towards Human-AI conversation use cases. Our model incorporates a safety risk taxonomy, a valuable tool for categorizing a specific set of safety risks found in LLM prompts (i.e., prompt classification). This taxonomy is also instrumental in classifying the responses generated by LLMs to these prompts, a process we refer to as response classification. For the purpose of both prompt and response classification, we have meticulously gathered a dataset of high quality. Llama Guard, a Llama2-7b model that is instruction-tuned on our collected dataset, albeit low in volume, demonstrates strong performance on existing benchmarks such as the OpenAI Moderation Evaluation dataset and ToxicChat, where its performance matches or exceeds that of currently available content moderation tools. Llama Guard functions as a language model, carrying out multi-class classification and generating binary decision scores. Furthermore, the instruction fine-tuning of Llama Guard allows for the customization of tasks and the adaptation of output formats. This feature enhances the model's capabilities, such as enabling the adjustment of taxonomy categories to align with specific use cases, and facilitating zero-shot or few-shot prompting with diverse taxonomies at the input. We are making Llama Guard model weights available and we encourage researchers to further develop and adapt them to meet the evolving needs of the community for AI safety.
  


## More Readings: 


### Not what you've signed up for: Compromising Real-World LLM-Integrated Applications with Indirect Prompt Injection
+ [Submitted on 23 Feb 2023 (v1), last revised 5 May 2023 (this version, v2)]
+ https://arxiv.org/abs/2302.12173
+ Kai Greshake, Sahar Abdelnabi, Shailesh Mishra, Christoph Endres, Thorsten Holz, Mario Fritz
+ Large Language Models (LLMs) are increasingly being integrated into various applications. The functionalities of recent LLMs can be flexibly modulated via natural language prompts. This renders them susceptible to targeted adversarial prompting, e.g., Prompt Injection (PI) attacks enable attackers to override original instructions and employed controls. So far, it was assumed that the user is directly prompting the LLM. But, what if it is not the user prompting? We argue that LLM-Integrated Applications blur the line between data and instructions. We reveal new attack vectors, using Indirect Prompt Injection, that enable adversaries to remotely (without a direct interface) exploit LLM-integrated applications by strategically injecting prompts into data likely to be retrieved. We derive a comprehensive taxonomy from a computer security perspective to systematically investigate impacts and vulnerabilities, including data theft, worming, information ecosystem contamination, and other novel security risks. We demonstrate our attacks' practical viability against both real-world systems, such as Bing's GPT-4 powered Chat and code-completion engines, and synthetic applications built on GPT-4. We show how processing retrieved prompts can act as arbitrary code execution, manipulate the application's functionality, and control how and if other APIs are called. Despite the increasing integration and reliance on LLMs, effective mitigations of these emerging threats are currently lacking. By raising awareness of these vulnerabilities and providing key insights into their implications, we aim to promote the safe and responsible deployment of these powerful models and the development of robust defenses that protect users and systems from potential attacks.
+ Subjects: Cryptography and Security (cs.CR); Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Computers and Society (cs.CY)

### A Survey on Large Language Model (LLM) Security and Privacy: The Good, the Bad, and the Ugly

+ [Submitted on 4 Dec 2023]
+ Large Language Models (LLMs), such as GPT-3 and BERT, have revolutionized natural language understanding and generation. They possess deep language comprehension, human-like text generation capabilities, contextual awareness, and robust problem-solving skills, making them invaluable in various domains (e.g., search engines, customer support, translation). In the meantime, LLMs have also gained traction in the security community, revealing security vulnerabilities and showcasing their potential in security-related tasks. This paper explores the intersection of LLMs with security and privacy. Specifically, we investigate how LLMs positively impact security and privacy, potential risks and threats associated with their use, and inherent vulnerabilities within LLMs. Through a comprehensive literature review, the paper categorizes findings into "The Good" (beneficial LLM applications), "The Bad" (offensive applications), and "The Ugly" (vulnerabilities and their defenses). We have some interesting findings. For example, LLMs have proven to enhance code and data security, outperforming traditional methods. However, they can also be harnessed for various attacks (particularly user-level attacks) due to their human-like reasoning abilities. We have identified areas that require further research efforts. For example, research on model and parameter extraction attacks is limited and often theoretical, hindered by LLM parameter scale and confidentiality. Safe instruction tuning, a recent development, requires more exploration. We hope that our work can shed light on the LLMs' potential to both bolster and jeopardize cybersecurity.

### Baseline Defenses for Adversarial Attacks Against Aligned Language Models
+ https://github.com/neelsjain/baseline-defenses 


(Page 10-22)
### Experiments
+ Due to the lack of standardized taxonomies, different models were trained on and tested on different datasets all with their own taxonomy.
+ Llama Guard is evaluated on two axes:
    1. In-domain performance on its **own datasets and taxonomy** (on-policy setting)
    2. Adaptability to **other taxonomies** (off-policy setting)
#### Evaluation Methodology
+ To evaluate on several datasets, all with different taxonomies, different bars and without clear mapping, there are three techniques used to subjectively evaluate the models.
    1. **Overall binary classification for APIs that provide per-category output**: assigns positive label if any positive label is predicted, regardless of whether it aligns with GT target category.
        * ex: text1 -> Predicted: violence&hate GT: sexual content Result: unsafe, right prediction
    2. **Per-category binary classification via 1-vs-all**: unsafe only if violates target category. (This method focuses on models' ability to **predict the category right**, rather than differentiate safe and unsafe.)
        * ex: text2 -> Predicted: violence&hate GT: sexual context Result: safe, wrong prediction 
        * ex: text2 -> Predicted: violence&hate GT: violence&hate Result: unsafe, right prediction 
    3. **Per-category binary classification via 1-vs-benign**: only benign labels are considered negative, removes hard negatives. (If a positively labeled sample belonging to a category that is not the target category, it will be dropped)
        * ex: calculating precision=TP/(TP+FP), less likely to predict false positive as less actual negative exists
 + The second method is used for evaluating Llama Guard both for the internal test set and for other datasets; the authors follow the third approach for all the baseline APIs that they evaluate.
 
#### Benchmarks and Baselines
+ **Benchmarks (datasets):**
    1. ToxicChat: 10k, real-world user-AI interactions.
    2. OpenAI Moderation Evaluation Dataset: 1,680 prompt examples, labeled according the OpenAI moderation API taxonomy
+ **Baselines (models):**
    1. OpenAI Moderation API: GPT-based, multi-label fine-tuned classifier 
    2. Perspective API: for online platforms and publishers
    3. Azure AI Content Safety API: Microsoft multi-label classifier (inapplicable for AUORC)
    4. GPT-4: content moderation via zero-shot prompting (inapplicable for AUORC)
+ **OpenAI Moderation Evaluation Dataset**<br />
    <img src="docs/images/OpenAIModeration.jpg " width=65% height=65%>

#### Metrics
+ The authors use the **area under the precision-recall curve (AUPRC)** as the evaluation metrics, which reflects the trade-off between precision and recall.<br />
    <img src="docs/images/AUPRC.jpg " width=80% height=80%>

#### Results
+ **General**<br />
    <img src="docs/images/Results1.jpg " width=80% height=80%>
+ **Per category**<br />
    <img src="docs/images/Results2.jpg " width=80% height=80%>
+ Llama Guard has the **best scores** on its **own dataset**, both in general and for each category.
+ Llama Guard achieves **similar performace** to OpenAI's API on its **Moderation dataset**, and has the **highest score** on **ToxicChat**.

#### More on Adaptability
+ **Adaptability via Prompting**<br />
    <img src="docs/images/Adaptability1.jpg " width=80% height=80%><br />
    Few-shot prompting **improves** Llama Guard's performance on OpenAI Mod dataset per category, compared to zero-shot prompting.

+ **Adaptability via Fine-tuning**<br />
    <img src="docs/images/Adaptability2.jpg " width=80% height=80%><br />
    Llama Guard needs only **20%** of the ToxicChat dataset to perform comparably with Llama2-7b trained on **100%** of the ToxicChat dataset
 
 ### Purple Llama
 + Under the purview of Purple Llama, developers can use open trust and safety tools and assessments to properly implement generative AI models and experiences.
 + Somewhere between red(attack) and blue(defensive) team, purple is the middle color, is a collaborative approach to evaluating and mitigating potential risks
 + First industry-wide set of **cybersecurity safety evaluations** for LLMs
 + Tools and evaluations for **input/output safeguards**<br />
     <img src="docs/images/PurpleLlama.jpg " width=70% height=70%>
 
 ### References
 + https://platform.openai.com/docs/guides/moderation/
 + https://medium.com/@douglaspsteen/precision-recall-curves-d32e5b290248
 + https://ai.meta.com/blog/purple-llama-open-trust-safety-generative-ai/
 
