---
layout: post
title: Retrieval Augmented FMs 
lecture: S0-Intro
lectureVersion: next
extraContent: 
notes: team-1
video:  team-2
tags:
- 1Basic
---

In this session, our readings cover: 

## Readings: 
  ### Retrieval-Augmented Generation for Large Language Models: A Survey
  + https://arxiv.org/abs/2312.10997
  + Large language models (LLMs) demonstrate powerful capabilities, but they still face challenges in practical applications, such as hallucinations, slow knowledge updates, and lack of transparency in answers. Retrieval-Augmented Generation (RAG) refers to the retrieval of relevant information from external knowledge bases before answering questions with LLMs. RAG has been demonstrated to significantly enhance answer accuracy, reduce model hallucination, particularly for knowledge-intensive tasks. By citing sources, users can verify the accuracy of answers and increase trust in model outputs. It also facilitates knowledge updates and the introduction of domain-specific knowledge. RAG effectively combines the parameterized knowledge of LLMs with non-parameterized external knowledge bases, making it one of the most important methods for implementing large language models. This paper outlines the development paradigms of RAG in the era of LLMs, summarizing three paradigms: Naive RAG, Advanced RAG, and Modular RAG. It then provides a summary and organization of the three main components of RAG: retriever, generator, and augmentation methods, along with key technologies in each component. Furthermore, it discusses how to evaluate the effectiveness of RAG models, introducing two evaluation methods for RAG, emphasizing key metrics and abilities for evaluation, and presenting the latest automatic evaluation framework. Finally, potential future research directions are introduced from three aspects: vertical optimization, horizontal scalability, and the technical stack and ecosystem of RAG.

  ### Retrieval-Augmented Domain Adaptation of Language Models

  ### Making Retrieval Augmented Generation Fast
  + https://www.pinecone.io/learn/fast-retrieval-augmented-generation/

    ### LlamaIndex  
  + https://docs.llamaindex.ai/en/stable/
  LlamaIndex supports Retrieval-Augmented Generation (RAG). Instead of asking LLM to generate an answer immediately, LlamaIndex:
 retrieves information from your data sources first, / adds it to your question as context, and / asks the LLM to answer based on the enriched prompt.
