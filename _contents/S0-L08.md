---
layout: post
title: FM copyright infrigement 
lecture: S0-Intro
lectureVersion: next
extraContent: 
nnotes: team-1
video:  team-2
tags:
- 1Basic
---

In this session, our readings cover: 

## Readings: 
### Membership Inference Attacks against Language Models via Neighbourhood Comparison
https://aclanthology.org/2023.findings-acl.719/ 

###  Copyright Plug-in Market for The Text-to-Image Copyright Protection
  https://openreview.net/forum?id=pSf8rrn49H 

### The Pile: An 800GB Dataset of Diverse Text for Language Modeling
  + https://arxiv.org/abs/2101.00027
  + Recent work has demonstrated that increased training dataset diversity improves general cross-domain knowledge and downstream generalization capability for large-scale language models. With this in mind, we present \textit{the Pile}: an 825 GiB English text corpus targeted at training large-scale language models. The Pile is constructed from 22 diverse high-quality subsets -- both existing and newly constructed -- many of which derive from academic or professional sources. Our evaluation of the untuned performance of GPT-2 and GPT-3 on the Pile shows that these models struggle on many of its components, such as academic writing. Conversely, models trained on the Pile improve significantly over both Raw CC and CC-100 on all components of the Pile, while improving performance on downstream evaluations. Through an in-depth exploratory analysis, we document potentially concerning aspects of the data for prospective users. We make publicly available the code used in its construction.


  