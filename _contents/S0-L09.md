---
layout: post
title: FM privacy leakage issues 
lecture: S0-Intro
lectureVersion: next
extraContent: 
notes: team-1
video: team-4
tags:
- 1Basic
---

In this session, our readings cover: 

## Readings: 

### Privacy Risks of General-Purpose Language Models
  + https://ieeexplore.ieee.org/abstract/document/9152761


### ProPILE: Probing Privacy Leakage in Large Language Models
+ https://arxiv.org/abs/2307.01881
+ Siwon Kim, Sangdoo Yun, Hwaran Lee, Martin Gubri, Sungroh Yoon, Seong Joon Oh
The rapid advancement and widespread use of large language models (LLMs) have raised significant concerns regarding the potential leakage of personally identifiable information (PII). These models are often trained on vast quantities of web-collected data, which may inadvertently include sensitive personal data. This paper presents ProPILE, a novel probing tool designed to empower data subjects, or the owners of the PII, with awareness of potential PII leakage in LLM-based services. ProPILE lets data subjects formulate prompts based on their own PII to evaluate the level of privacy intrusion in LLMs. We demonstrate its application on the OPT-1.3B model trained on the publicly available Pile dataset. We show how hypothetical data subjects may assess the likelihood of their PII being included in the Pile dataset being revealed. ProPILE can also be leveraged by LLM service providers to effectively evaluate their own levels of PII leakage with more powerful prompts specifically tuned for their in-house models. This tool represents a pioneering step towards empowering the data subjects for their awareness and control over their own data on the web.

### Are Large Pre-Trained Language Models Leaking Your Personal Information?
+ https://arxiv.org/abs/2205.12628
+ Jie Huang, Hanyin Shao, Kevin Chen-Chuan Chang
Are Large Pre-Trained Language Models Leaking Your Personal Information? In this paper, we analyze whether Pre-Trained Language Models (PLMs) are prone to leaking personal information. Specifically, we query PLMs for email addresses with contexts of the email address or prompts containing the owner's name. We find that PLMs do leak personal information due to memorization. However, since the models are weak at association, the risk of specific personal information being extracted by attackers is low. We hope this work could help the community to better understand the privacy risk of PLMs and bring new insights to make PLMs safe.


